---
title: |
  | PROPOSAL
  | 
  | 
  |
  |
  |
  | **Bayesian Evidence Synthesis**
  |
  |
  |
  |
  |
  |
  |
author: |
  | Thom Volker, 5868777
  |
  |
  | Supervisor: Irene Klugkist
  |
  |
  |
  | *Methodology and Statistics for the Behavioural, Biomedical and Social Sciences*
  |
  | *Utrecht University*
  |
  |
  |
  | `r format(Sys.time(), '%B %d, %Y')`
  |
  |
  | `r paste0("Word count: ", wordcountaddin::word_count())`
output: 
  bookdown::pdf_document2:
    number_sections: true
    df_print: kable
    highlight: pygments
    toc: false
mainfont: Calibri
sansfont: Calibri
linestretch: 2
fontsize: 10pt
params: 
  !r Sys.setlocale("LC_TIME", "C")
date: 
indent: true
tables: true
header-includes:
  - \usepackage{caption}
  - \usepackage{xspace}
  - \usepackage{tikz}
  - \usetikzlibrary{shapes.geometric,arrows}
  - \def\TikZ{Ti\emph{k}Z\ }
bibliography: "/Users/thomvolker/Documents/Master_Thesis/thesis_literature.bib"
csl: "/Users/thomvolker/Documents/styles/apa-6th-edition.csl"
link-citations: yes
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\newpage

\definecolor{darkgreen}{rgb}{0.0, 0.44, 0.0}

<!--
\noindent
\textcolor{darkgreen}{Ik vraag me af of dit niet een te uitgebreide inleiding voor een proposal is, omdat de limiet maar 750 woorden is. Ik kan dit ook bewaren voor de daadwerkelijke inleiding, en het proposal heel kort inleiding met iets als: onderzoekers maken nou eenmaal bewust en onbewust verschillende keuzes met eenzelfde onderzoeksvraag. Wanneer dit het geval is, is het lastig om het bewijs te aggregeren, omdat meta-analyse hier niet goed toe in staat is. Een alternatieve methode is BES, die gebruikt kan worden om ... }
-->

# Introduction

<!--\textcolor{darkgreen}{Word count is inclusief comments, zonder comments is de word count 479.}-->

In recent years, a meta-analytic way of thinking has been advocated in the scientific community, an approach that is grounded in the belief that a single study is merely contributing to a larger body of evidence [@cumming_new_2014]. Additionally, the importance of replication has been legitimately supported [e.g., @open_science_collab_2015; @baker_reproducibility_2016; @brandt_et_al_replication_2014]. However, most of the attention has been focused on studies that are highly similar, using an identical methodology and research design. These studies, commonly referred to as exact, direct or close replications, are merely concerned with the statistical reliability of the results. Unfortunately, if the results of these studies depend on methodological flaws, inferences from all studies will lead to suboptimal or invalid conclusions [@munafo_robust_2018]. A safeguard against this deficiency is available in the form of conceptual replications, which primarily assess the validity of a study. That is, conceptual replications are a way of investigating whether the initial conclusions hold under different conditions, using varying measurement instruments or choosing different operationalizations. 
 
Consequently, multiple studies regarding the same hypotheses arise and as per the cumulative nature of science, synthesizing the results is required to build a robust and solid body of evidence. As these conceptual replications employ fundamental between-study differences, established synthesizing methods as (Bayesian) meta-analysis and Bayesian updating are not applicable [@lipsey_wilson_2001; @sutton_bayesian_meta2001; @schonbrodt_sequential_2017]. These methods are restricted to combining parameter estimates that (i) share a common scale, and (ii) result from analyses with identical functional forms. To overcome these difficulties, @kuiper_combining_2013 proposed to use Bayesian Evidence Synthesis (BES), which allows researchers to pool evidence for a specific hypothesis over multiple studies, even if the studies have seemingly incompatible designs.

The use of Bayes Factors [@kass_raftery_bayes_factors_1995] is at the very heart of BES. First, one proceeds by constructing study-specific hypotheses that reflect a more general hypothesis (i.e., scientific theory). Since the studies might differ conceptually, the hypotheses are allowed to vary over the studies, provided that the hypotheses address the same general hypothesis. Subsequently, the support for each of these study-specific hypotheses can be expressed in terms of a Bayes Factor. Bayes Factors render the support of the hypothesis at hand, relative to an alternative hypothesis, for which conveniently an unconstrained or complement hypothesis can be selected. Loosely speaking, the Bayes Factor expresses how much more likely the hypothesis at hand is, compared to the chosen alternative. Ultimately, the individual Bayes Factors can be multiplied, to express the support for the overall hypothesis in one measure of evidence [@kuiper_combining_2013].

# Approach

In this study, we will build upon previous work on BES by @kuiper_combining_2013 and @behrens_2019 who presented a proof of concept that has been followed by actual implementation of the method [e.g., @zondervan_robust_2020; @zondervan_parental_2019]. @behrens_2019 showed that BES functions adequately when inequality constrained, that is, informative, hypotheses are evaluated, but shows problems when interest is in evaluation of equality constrained hypotheses [for a thorough overview of the distinction, see @hoijtink_informative_2012]. These problems become especially apparent when a given study lacks statistical power, that is, when the sample size is small relative to the effect size. Additionally, Bayes Factors are known to be highly dependent on the complexity of a given hypothesis [i.e., the number of parameters that are addressed by that hypothesis; @klugkist_inequality_2005; @mulder_equality_2010]. If conceptually similar studies assess hypotheses that differ in the number of addressed parameters, synthesizing the results of the studies may become problematic. 

In the current study, we will aim at revealing under which circumstances BES performs adequately, and under which circumstances the method performs unsatisfactorily. We will do so by employing multiple simulations, in which samples are generated and analysed by means of multiple statistical models. Specifically, data will be generated and analysed according to multivariable linear, logit and probit models, driven by actual sociological research problems [e.g., @behrens_2019; @kuiper_combining_2013; @buskens_raub_embedded_2002]. In accordance with @behrens_2019, we will let sample sizes vary between $n = 25$ to $n = 500$ in steps of $25$, and small, medium and large effects as defined by @cohen_1988 will be considered. The performance of BES will be evaluated through the true hypothesis rate (THR). Desirably, the THR will converge towards the upper boundary, which is dependent upon the exact specification of the hypotheses. Ethical consent has been provided by the FERB.

<!--Nevertheless, research concerning the performance of BES has been limited to relatively simple settings, and more extensive studies are required to enhance the applicability of the method.-->

<!--
Although BES has been applied in multiple studies [e.g., @zondervan_robust_2020; @zondervan_parental_2019], research into the performance of the method is still limited. Besides @kuiper_combining_2013, the performance of BES has been investigated by @behrens_2019, who showed that BES tends to perform better when only inequality constrained, that is, informative, hypotheses are considered, as compared to equality constrained hypotheses [for a thorough overview about the distinction, see @hoijtink_informative_2012]. However, both studies predominantly serve as a proof of concept, and more complex simulations should validate and enhance the applicability of the method. \textcolor{darkgreen}{Ik wil niemand beledigen natuurlijk, maar de simulatiesettings daar zijn relatief simpel, en wij willen juist verder de diepte in. Als dit te sterk uitgedrukt is hoor ik het graag.} Namely, Bayes Factors are highly dependent on the complexity of the hypothesis [i.e., the number of parameters that are addressed by the hypothesis; @klugkist_inequality_2005], which may pose yet unaddressed problems when the study-specific hypotheses involve differing numbers of parameters. Additionally, since Bayes Factors are highly dependent on the sample size and effect size, the effect of having, at least, one underpowered study in the set of studies should receive considerable scrutiny. To address these questions, conform the design of @behrens_2019, varying statistical models (Poisson, logit, probit and linear regression), sample sizes ($n = 25, n = 50,~ ..., n = 500$) and effect sizes as defined by @cohen_1988 will be adopted. The performance of BES will be evaluated by means of the True Hypothesis Rate (THR), which quantifies the proportion of times BES is able to identify the correct hypothesis. A THR above the threshold of 0.80 will be seen as convincing evidence. \textcolor{darkgreen}{Mijn mentor vond het belangrijk om ook evaluatiecriteria van tevoren te bepalen. Hoe staan jullie hierin? Ik heb namelijk niet het idee dat het heel gangbaar is om van tevoren een THR te bepalen die gebruikt gaat worden, vrij veel is denk ik afhankelijk van wat de algemene tendens in bepaalde situaties is, en dat is niet per se direct uit te drukken in een THR.} 
-->

<!--

However, when researchers conceptually replicate a study, fundamental differences between the study-designs may occur. Similar between-study discrepancies may occur when researchers unintentionally make different data-analytic choices, a situation that is commonly referred to as the garden of forking paths [@gelman_loken_garden_2014]. Unfortunately, established methods as (Bayesian) meta-analysis and Bayesian sequential updating are inappropriate to pool estimates from incompatible study designs [@lipsey_wilson_2001; @sutton_bayesian_meta2001; @schonbrodt_sequential_2017], due to the requirement that the estimated parameters (i) share a common scale, and (ii) arise from analyses with identical functional forms. . 

In such instances, established methods as (Bayesian) meta-analysis and Bayesian updating are deemed inappropriate, due to the requirements that parameter estimates (i) share a common scale, and (ii) result from analyses with identical functional forms. 

ly, discrepancies between studies may occur when researchers unintentionally
The same difficulties may apply when researchers unintentionally make different data-analytic choices, a situation that is commonly referred to as the garden of forking paths [@gelman_loken_garden_2014]. 
The same holds when researchers unintentionally make different data-analytic choices,   study-designs may be Conceivably, operationalizations of theoretical constructs and consequently the applied methods may vary over the studies when conceptual replications are administered. 

if studies are replicated conceptually. 
However, when studies are replicated conceptually, conceivably operationalizations vary over the studies
However, when studies differ conceptually, as conceptual replications impdifferent statistical modelsestablished methods as (Bayesian) meta-analysis and Bayesian sequential updating are not applicable, due to these methods requiring 

When the studies are highly similar, established methods as (Bayesian) meta-analysis and Bayesian updating can be used to pool the results [@lipsey_wilson_2001; @sutton_bayesian_meta2001; @schonbrodt_sequential_2017]. However, when researchers conceptually replicate an earlier study, fundamental differences between the study-designs may occur. The same holds when researchers unintentionally make different data-analytic choices, a situation that is referred to as the garden of forking paths [@gelman_loken_garden_2014]. Under these circumstances, conventional synthesizing methods do not suffice, because these are restricted to combining parameter estimates that (i) share a common scale, and (ii) result from analyses with identical functional forms. Consequently, @kuiper_combining_2013 proposed to use Bayesian Evidence Synthesis (BES), which allows researchers to pool the evidence for a specific hypothesis over multiple studies, even if the studies have seemingly incompatible designs. 

-->


<!--
Building on these studies, the performance of BES will be addressed in situations where datasets are simulated according to multiple statistical models (e.g., linear, logit, probit or Poisson models). However, the fact that Bayes Factors are highly dependent on the complexity of the hypothesis (i.e., the number of parameters that are addressed within the hypothesis) has been unrecognized
As in previous research, the samples sizes and effect sizes will vary over the simulations, but additionally, careful attention will be paid to scenarios where the number of constraints in the study-specific hypotheses differ. 

multiple statistical models are used to 

 By performing multiple simulations, the current study aims at extending the current state of knowledge in this area. More specifically, the performance of BES will be scrutinized in situations where the number of constraints in the study-specific hypotheses differ. 
-->




<!--
DISCUSSIEPUNTEN VOOR IN VOLLEDIGE INTRO OF DISCUSSIE
- GARDEN OF FORKING PATHS IS OOK EEN REDEN DAT BES NUTTIG IS



<!--The importance of replication in science has been legitimately supported during recent years [e.g., @open_science_collab_2015; @baker_reproducibility_2016; @brandt_et_al_replication_2014]. <!-- {\textcolor{darkgreen}{MISSCHIEN HIER NOG EEN VOORBEELD AAN KOPPELEN, MAAR WORDT MISSCHIEN OOK TE VEEL (AFHANKELIJK VAN HOE DUIDELIJK HET ZONDER VOORBEELD IS)} --> <!--However, most of the attention has been focused on exact, or close replications, that is, replications concerned with the statistical reliability of the results. Unfortunately, if results from any initial study depend on methodological flaws, inferences from exact replications of this study will reproduce these methodological artefacts, leading to suboptimal or invalid conclusions [@munafo_robust_2018]. A solution for this issue is available in the form of conceptual replications, which are merely about the validity of the study. That is, conceptual replications are a way of investigating whether the initial conclusions hold under different conditions, such as a different operationalization of the construct of interest or a different measurement instrument. Also, next to purposefully alter aspects of a certain study as done in conceptual replications, researchers may make different, but justifiable, choices when it comes to analysing the data. This situation, which is also commonly referred to as *the garden of forking paths*, in combination with intentional replication research, leads to the fact that there are often multiple studies assessing the same underlying research question.

Once several studies have been completed, several methods are available to combine the results from all studies into a single measure of evidence with regard to the hypothesis at hand. However, the best-known methods for evidence aggregation, meta-analysis and Bayesian updating, are not appropriate when the goal is to pool the results from studies with different designs or different statistical methods [@lipsey_wilson_2001; ]. 

<!--
Additionally, researcher can analyse a single dataset in a variety of justifiable ways

researchers may also unconsciously use different study designs

\textcolor{darkgreen}{EVENTUEEL AANVULLEN DAT ONDERZOEKERS OOK ONBEWUST VERSCHILLENDE KEUZES KUNNEN MAKEN, DIT KAN OOK WEER PROBLEMEN OPLEVEREN VOOR SAMENVOEGEN RESULTATEN}. 


It is expected that if there actually exists a relevant effect in the population, is should become apparent under a variety of justifiable methods and operationalizations.



\bigskip
\bigskip

Multiple researcher may make a variety of justifiable decisions with regard to the design of a study, all leading to deviating results, a tendency that is commonly referred to as *the garden of forking paths*.


\bigskip
\bigskip



The effects estimated in these replications can be pooled, eventually with other studies with a similar design, by means of meta-analysis or Bayesian sequential updating, so that a researcher is able to obtain a more precise estimate of the population parameter of interest. Although the initiative to directly replicate and/or combine similar studies is commendable, this approach lacks depth


<!--of such replications is commendable


Although this is a commendable initiative, direct replications lack some of the benefits offered by conceptual replications.


have severe shortcomings by themselves, as illustrated by @munafo_robust_2018. Most importantly, the authors stress that if researcher make use of flawed methods or study designs, methodological artefacts could be taken as truth. 


<!--
Although every single properly designed scientific study is a step in the right direction, a single study is also always insufficient

As of today, several methods for aggregating scientific evidence have been proposed. Examples are, but are not limited to, conventional (study-level) meta-analysis, individual-level meta-analysis and Bayesian updating. The advantages of such methods are numerous: (i) they enable researchers to establish effects over different studies, (ii) 

As of today, many methods for aggregating scientific evidence have been proposed. Two well-known examples are meta-analysis and Bayesian sequantial updating. Both methods aim at combining the multiple sources of evidence, either from multiple studies, or from collecting additional data.
-->


# References

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\noindent

<div id="refs"></div>
